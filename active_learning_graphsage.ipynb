{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55c465a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/geo/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import entropy\n",
    "\n",
    "import torch\n",
    "from torch_geometric.loader import DataLoader, ClusterData, ClusterLoader, NeighborLoader, DataListLoader\n",
    "from torch_geometric.data import Batch\n",
    "from torch_geometric.nn import DataParallel\n",
    "import graphClasses as gf\n",
    "import edgePruning as ep\n",
    "import visFunctions as vf\n",
    "\n",
    "import json\n",
    "import pickle\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e3c54e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's use 4 GPUs!\n"
     ]
    }
   ],
   "source": [
    "print(f\"Let's use {torch.cuda.device_count()} GPUs!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4284f057",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_seed = 830\n",
    "\n",
    "np.random.seed(my_seed)\n",
    "torch.manual_seed(my_seed)\n",
    "torch.cuda.manual_seed(my_seed)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c097d9a1",
   "metadata": {},
   "source": [
    "# Active Learning\n",
    "\n",
    "Learn models as data is accessed from the pool via the specified query selection method.\n",
    "\n",
    "The 100 meshes in the MPI dataset is actually comprised of 10 samples each wih 10 poses. To reduce computational cost but to preserve logic given the nature of this dataset, our active learning queries all poses from the patient selected at each round. Specifically, we sum the disagreement from all poses for each patient, and select the patient with highest total disagreement across poses. We query all poses from this patient for the next round of active learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89eb8f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NewOnlineLearning(dim_h, dim_in, model_dir, pred_dir, gif_dir, start_n, end_n, num_instances, num_poses, querySelection, X, epochs, rand_i,\n",
    "                      load_model:str=None,mc_repeats:int=10,mc_prob:float=0.3,):\n",
    "    train_accs = np.zeros((num_instances+1))\n",
    "    train_dices = np.zeros((num_instances+1))\n",
    "    train_ious = np.zeros((num_instances+1))\n",
    "    train_losses = np.zeros((num_instances+1))\n",
    "\n",
    "    unobs_accs = np.zeros((num_instances+1))\n",
    "    unobs_dices = np.zeros((num_instances+1))\n",
    "    unobs_ious = np.zeros((num_instances+1))\n",
    "    unobs_losses = np.zeros((num_instances+1))\n",
    "    \n",
    "    X_L, X_U = X[:start_n], X[start_n:]\n",
    "    print('X_L,X_U',len(X_L), len(X_U))\n",
    "    print(len(X_L[0]), len(X_U[0]))\n",
    "\n",
    "    if run_i == 0:\n",
    "        idx_list = list(range((10-start_n)*num_poses))\n",
    "#         print(len(idx_list))\n",
    "        imgobj = vf.ImageObject(gif_dir,idx_list)\n",
    "\n",
    "    model_fname = f'/al_base'\n",
    "    model_path = model_dir+model_fname\n",
    "\n",
    "    model = gf.GraphSAGE(dim_h, dim_in, 12)\n",
    "    model = DataParallel(model)\n",
    "    model.to(device)\n",
    "\n",
    "    print(\"=\"*30, f'Starting {str(querySelection).upper()}','='*30)\n",
    "\n",
    "    print(f'Training/Testing on Initial sampling..........')\n",
    "    \n",
    "    # Train/test first samples\n",
    "    #split data in test, train and validation\n",
    "    val_train_split = len(X_L)//3\n",
    "    val_lists = X_L[:val_train_split]\n",
    "    val_list = [pose for patient_poses in val_lists for pose in patient_poses]\n",
    "    # print('Constructing validation data.')\t\n",
    "    # val_batch = Batch.from_data_list(val_list)\n",
    "    # val_data = DataLoader(val_batch, batch_size=1)\n",
    "    val_data = DataListLoader(val_list, batch_size=6, shuffle=True)\n",
    "\n",
    "    train_lists = X_L[val_train_split:]\n",
    "    train_list = [pose for patient_poses in train_lists for pose in patient_poses]\n",
    "    # print('Constructing training data.')\n",
    "    # train_batch = Batch.from_data_list(train_list)\n",
    "    # train_data = DataLoader(train_batch, batch_size=1)\n",
    "    train_data = DataListLoader(train_list, batch_size=6, shuffle=True)\n",
    "    \n",
    "    test_list = [pose for patient_poses in X_U for pose in patient_poses]\n",
    "    # print('Constructing test data.')\n",
    "    # test_data = DataLoader(test_list)\n",
    "    test_data = DataListLoader(test_list, batch_size=6, shuffle=True)\n",
    "    # train_data = DataListLoader(train_list, batch_size=1, shuffle=True)\n",
    "    # val_data = DataListLoader(val_list, batch_size=1, shuffle=True)\n",
    "    \n",
    "    print('Training model...')\n",
    "    start_time = time.time()\n",
    "    train_loss, train_acc, train_iou, train_dice, model = gf.train_model(model, device, train_data, val_data,epochs=epochs)\n",
    "    train_accs[0] = train_acc\n",
    "    train_dices[0] = train_dice\n",
    "    train_losses[0] = train_loss\n",
    "    train_ious[0] = train_iou\n",
    "    print(f'Total time taken for training: {time.time()-start_time:.4f} seconds')\n",
    "    \n",
    "    print('Saving model...')\n",
    "    gf.save_model(model, model_path)\n",
    "    \n",
    "    print('Running test graphs.')\n",
    "    loss, acc, iou, dice, predict_probs = gf.test_model(model, device, test_data)\n",
    "    unobs_accs[0] = acc\n",
    "    unobs_dices[0] = dice\n",
    "    unobs_losses[0] = loss\n",
    "    unobs_ious[0] = iou\n",
    "    # print('passive predict_probs type',type(predict_probs))\n",
    "    print(predict_probs.shape)\n",
    "    print(f'Testing loss: {loss:.2f}')\n",
    "    print(f'Testing accuracy: {acc:.2f}')\n",
    "    print(f'Testing iou: {iou:.2f}')\n",
    "    print(f'Testing dice: {dice:.2f}')\n",
    "    print('-'*40)\n",
    "\n",
    "    \n",
    "    f = open(os.path.join(pred_dir,f'al_base.pckl'),'wb')\n",
    "    pickle.dump(predict_probs, f)\n",
    "    f.close()\n",
    "\n",
    "\n",
    "    if run_i == 0:\n",
    "        if querySelection is not None:\n",
    "            imgobj.create_images(test_list, predict_probs, num_instances, num_poses, 'initial', querySelection.upper())\n",
    "            imgobj.create_images(test_list, predict_probs, num_instances, num_poses, 'initial', 'Passive')\n",
    "\n",
    "    for instance_i, split_i in enumerate(range(start_n,end_n)):\n",
    "        if querySelection is not None:\n",
    "            print(f'\\n>>> Querying instance {instance_i+1} with {str(querySelection).upper()}..........')\n",
    "        else:\n",
    "            print(f'\\n>>> Querying instance {instance_i+1} with Passive..........')\n",
    "\n",
    "        if querySelection == None:\n",
    "\n",
    "            model_fname = f'/al_instance{instance_i}'\n",
    "            model_path = model_dir+model_fname\n",
    "\n",
    "            # Passive learning\n",
    "            X_L = X[:split_i+1]\n",
    "            X_U = X[split_i+1:]\n",
    "            xstar_i = 0\n",
    "\n",
    "            print(f'PASSIVE QUERIED INDEX: {xstar_i}')        \n",
    "            \n",
    "            # unobs_accs[instance_i] = acc\n",
    "\n",
    "        elif querySelection == \"qbc\":\n",
    "            \n",
    "            ####################################\n",
    "            committee_size = 3\n",
    "            \n",
    "            labeled_list = [pose for patient_poses in X_L for pose in patient_poses]\n",
    "            \n",
    "            X_Ls = Bootstrap(committee_size, labeled_list)\n",
    "            print(len(X_Ls), len(X_Ls[0]), type(X_Ls[0][0]))\n",
    "\n",
    "            print(\"Training query committee.....\")\n",
    "            xstar_i, _, _, consensus_probs = QBC(dim_h, dim_in, device, X_Ls, X_U, instance_i, epochs)\n",
    "\n",
    "            print(f'QBC QUERIED INDEX: {xstar_i}')\n",
    "            # print('QBC consensus_probs type',type(consensus_probs))\n",
    "\n",
    "            \n",
    "            # test_list = [pose for patient_poses in X_U for pose in patient_poses]\n",
    "            # if run_i == 0:\n",
    "            #     imgobj.create_images(test_list, torch.tensor(consensus_probs), num_instances, num_poses, instance_i, querySelection)\n",
    "            #     imgobj.track_index(xstar_i,num_poses)\n",
    "                \n",
    "            #####################################\n",
    "            \n",
    "            X_L = X_L + [X_U[xstar_i]]\n",
    "            X_U = X_U[:xstar_i] + X_U[xstar_i+1:]\n",
    "            print(len(X_L), len(X_U))\n",
    "# #             print(len(X_L[0]), len(X_U[0]))\n",
    "\n",
    "        elif querySelection == \"mc\":\n",
    "            # model = gf.GraphSAGE_mc(dim_h, dim_in, 12, mc_prob=mc_prob)\n",
    "\n",
    "            xstar_i = MC_dropout(dim_h, dim_in, device, X_L, X_U, num_poses, epochs,mc_prob,mc_repeats)\n",
    "            print(f'MCD QUERIED INDEX: {xstar_i}')\n",
    "\n",
    "            X_L = X_L + [X_U[xstar_i]]\n",
    "            X_U = X_U[:xstar_i] + X_U[xstar_i+1:]\n",
    "\n",
    "        print('X_L,X_U',len(X_L), len(X_U))\n",
    "        # print(len(X_L[0]), len(X_U[0]))\n",
    "\n",
    "        model = gf.GraphSAGE(dim_h, dim_in, 12)\n",
    "        model = DataParallel(model)\n",
    "        model.to(device)\n",
    "\n",
    "        #split data in test, train and validation\n",
    "        val_train_split = len(X_L)//3\n",
    "        val_lists = X_L[:val_train_split]\n",
    "        val_list = [pose for patient_poses in val_lists for pose in patient_poses]\n",
    "        # print('Constructing validation data.')\t\n",
    "        # val_batch = Batch.from_data_list(val_list)\n",
    "        # val_data = DataLoader(val_batch, batch_size=1)\n",
    "        val_data = DataListLoader(val_list, batch_size=6, shuffle=True)\n",
    "\n",
    "        train_lists = X_L[val_train_split:]\n",
    "        train_list = [pose for patient_poses in train_lists for pose in patient_poses]\n",
    "        # print('Constructing training data.')\n",
    "        # train_batch = Batch.from_data_list(train_list)\n",
    "        # train_data = DataLoader(train_batch, batch_size=6)\n",
    "        train_data = DataListLoader(train_list, batch_size=6, shuffle=True)\n",
    "\n",
    "        test_list = [pose for patient_poses in X_U for pose in patient_poses]\n",
    "        # print('Constructing test data.')\n",
    "        # test_data = DataLoader(test_list)\n",
    "        test_data = DataListLoader(test_list, batch_size=6, shuffle=True)\n",
    "        \n",
    "        print('Training model...')\n",
    "        start_time = time.time()\n",
    "        # train_loss, train_acc, model = gf.train_model(model, device, train_data, val_data,epochs=epochs)\n",
    "        train_loss, train_acc, train_iou, train_dice, model = gf.train_model(model, device, train_data, val_data,epochs=epochs)\n",
    "        train_accs[instance_i] = train_acc\n",
    "        print(f'Total time taken for training: {time.time()-start_time:0.4f} seconds')\n",
    "        \n",
    "        print('Saving model...')\n",
    "        gf.save_model(model, model_path)\n",
    "        \n",
    "        print('Running test graphs.')\n",
    "        # loss, acc, predict_probs = gf.test_model(model, device, test_data)\n",
    "        loss, acc, iou, dice, predict_probs = gf.test_model(model, device, test_data)\n",
    "        # print('passive predict_probs type',type(predict_probs))\n",
    "        print(f'Testing loss: {loss:.2f}')\n",
    "        print(f'Testing accuracy: {acc:.2f}')\n",
    "        print(f'Testing iou: {iou:.2f}')\n",
    "        print(f'Testing dice: {dice:.2f}')\n",
    "        print('-'*40)\n",
    "\n",
    "\n",
    "        f = open(os.path.join(pred_dir,f'al_instance{instance_i}.pckl'),'wb')\n",
    "        pickle.dump(predict_probs, f)\n",
    "        f.close()\n",
    "        print(predict_probs.shape)\n",
    "\n",
    "        train_accs[instance_i+1] = train_acc\n",
    "        train_dices[instance_i+1] = train_dice\n",
    "        train_losses[instance_i+1] = train_loss\n",
    "        train_ious[instance_i+1] = train_iou\n",
    "\n",
    "        unobs_accs[instance_i+1] = acc\n",
    "        unobs_dices[instance_i+1] = dice\n",
    "        unobs_losses[instance_i+1] = loss\n",
    "        unobs_ious[instance_i+1] = iou\n",
    "\n",
    "        if run_i == 0:\n",
    "            imgobj.track_index(xstar_i,num_poses)\n",
    "            if querySelection is not None:\n",
    "                imgobj.create_images(test_list, predict_probs, num_instances, num_poses, instance_i, querySelection.upper())\n",
    "            else:\n",
    "                imgobj.create_images(test_list, predict_probs, num_instances, num_poses, instance_i, 'Passive')\n",
    "            # imgobj.track_index(xstar_i,num_poses)\n",
    "            \n",
    "            \n",
    "    return np.array([train_accs, train_dices, train_losses, train_ious]) , np.array([unobs_accs, unobs_dices, unobs_losses, unobs_ious])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e680d8cb",
   "metadata": {},
   "source": [
    "## Query By Committee\n",
    "\n",
    "Committee size is 3.\n",
    "\n",
    "**Bootstrapping.** Each bootstrapped dataset contains datasets of the same size as the training set at each round. Given a training set, $D$ of size $n$, Bagging involves creating $m$ new training sets of size $n$ by sampling from $D$ uniformly and with replacement (i.e., bootstrapping)\n",
    "\n",
    "**Quanify Disagreement across committee for each sample.** Our model outputs are log softmax probabilities for class, for each node, for each testing mesh; thus the committee is a tensor with shape [$m$, size of test set, number of nodes, number of classes]. In our case, the shape in the first round of active learning is [5, 50, 6890, 12]. The consensus is the average probability of each class and has shape [50, 6890, 12]. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2dff1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bootstrap(cmt_size, X_L):\n",
    "    # print(len(X_L))\n",
    "    X_Ls = []\n",
    "    numsamples = len(X_L)\n",
    "    for _ in range(cmt_size):\n",
    "        rand_i = np.random.choice(range(len(X_L)),size=numsamples,replace=True)\n",
    "        \n",
    "        X_Lbooted = [X_L[get_i] for get_i in rand_i]\n",
    "\n",
    "        X_Ls.append(X_Lbooted)\n",
    "    \n",
    "    return X_Ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23efb55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def QuantifyLargestPatientDisagreement(probs_a, consensus, num_poses):\n",
    "    '''\n",
    "    Our active learning queries all poses from the patient selected. Pilot uses 3 poses per patient.\n",
    "    '''\n",
    "    \n",
    "    # probs_a dimensions: number of committee members, number of testing meshes, number of nodes, number of classes\n",
    "    # consensus dimensions: number of testing meshes, number of nodes, number of classes\n",
    "    \n",
    "#     disagreements = np.zeros((probs_a.shape[:-1]))\n",
    "    \n",
    "    print('Calculating committee disagreement...')\n",
    "    print(probs_a.shape)\n",
    "    print(consensus.shape)\n",
    "    # print(np.sum(probs_a - np.expand_dims(consensus, axis=0)))\n",
    "    x = np.nan_to_num(probs_a / consensus)\n",
    "    disagreements = np.log(x) * probs_a\n",
    "    # disagreements = np.log(probs_a / consensus) * probs_a\n",
    "    print(disagreements.shape)\n",
    "    print(type(disagreements))\n",
    "    dis = np.sum(disagreements, axis=-1)\n",
    "    print(dis.shape)\n",
    "    dis2 = np.sum(dis, axis=0)\n",
    "    print(dis2.shape)\n",
    "    dis3 = np.sum(dis2, axis=-1)\n",
    "    print(dis3.shape)\n",
    "    print(dis3)\n",
    "    istar = np.argmax(dis3)\n",
    "    print(istar)\n",
    "    i_star = istar // num_poses\n",
    "    \n",
    "#     Calculating committee disagreement...\n",
    "#     (3, 50, 6890, 12)\n",
    "#     (50, 6890, 12)\n",
    "#     1.6078111064743439e-12\n",
    "#     (3, 50, 6890, 12)\n",
    "#     (3, 50, 6890)\n",
    "#     (50, 6890)\n",
    "#     (50,)\n",
    "#     [12919.23067897 13268.70436478 12768.37933145 13087.97039782\n",
    "#      12949.0351472  13002.58475957 12856.24169413 13334.29904475\n",
    "#      12834.23435822 13230.1138174  12891.58814957 13209.63356772\n",
    "#      12898.5849094  13238.97690748 12933.85406018 12871.79113584\n",
    "#      12816.01247897 13253.71351485 12727.60596102 13384.16116318\n",
    "#      12862.5137056  13159.11968156 12829.45098706 13170.80203524\n",
    "#      12887.40786216 12865.17355183 12775.07284143 13219.37530298\n",
    "#      12629.2226568  13337.91849373 12739.52346283 13126.33611794\n",
    "#      12973.85461498 13142.88021733 12769.79374444 12892.50038965\n",
    "#      12756.68478372 13331.63329016 12815.43654144 13258.23586064\n",
    "#      12873.13341915 13089.28156523 12816.48535762 13246.32550649\n",
    "#      12904.9296742  12894.91374903 12792.88232696 13576.56582084\n",
    "#      12693.39705421 13276.45955528]\n",
    "#     47\n",
    "    \n",
    "\n",
    "    return int(i_star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7bb9bba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def QuantifyLargestDisagreement(probs_a, consensus, num_poses):\n",
    "    '''\n",
    "    Our active learning queries all poses from the patient selected. Pilot uses 3 poses per patient.\n",
    "    '''\n",
    "    \n",
    "    # probs_a dimensions: number of committee members, number of testing meshes, number of nodes, number of classes\n",
    "    # consensus dimensions: number of testing meshes, number of nodes, number of classes\n",
    "    \n",
    "#     disagreements = np.zeros((probs_a.shape[:-1]))\n",
    "    \n",
    "    print('Calculating committee disagreement...')\n",
    "    # print(probs_a.shape)\n",
    "    # print(consensus.shape)\n",
    "    # print(np.sum(probs_a - np.expand_dims(consensus, axis=0)))\n",
    "    x = np.nan_to_num(probs_a / consensus)\n",
    "    disagreements = np.log(x) * probs_a\n",
    "    # disagreements = np.log(probs_a / consensus) * probs_a\n",
    "    # print(disagreements.shape)\n",
    "    # print(type(disagreements))\n",
    "    dis = np.sum(disagreements, axis=-1)\n",
    "    # print(dis.shape)\n",
    "    dis2 = np.sum(dis, axis=0)\n",
    "    # print(dis2.shape)\n",
    "    dis3 = np.sum(dis2, axis=-1)\n",
    "    # print(dis3.shape)\n",
    "    # print(dis3)\n",
    "    istar = np.argmax(dis3)\n",
    "    # print(istar)\n",
    "    i_star = istar // num_poses\n",
    "    \n",
    "#     Calculating committee disagreement...\n",
    "#     (3, 50, 6890, 12)\n",
    "#     (50, 6890, 12)\n",
    "#     1.6078111064743439e-12\n",
    "#     (3, 50, 6890, 12)\n",
    "#     (3, 50, 6890)\n",
    "#     (50, 6890)\n",
    "#     (50,)\n",
    "#     [12919.23067897 13268.70436478 12768.37933145 13087.97039782\n",
    "#      12949.0351472  13002.58475957 12856.24169413 13334.29904475\n",
    "#      12834.23435822 13230.1138174  12891.58814957 13209.63356772\n",
    "#      12898.5849094  13238.97690748 12933.85406018 12871.79113584\n",
    "#      12816.01247897 13253.71351485 12727.60596102 13384.16116318\n",
    "#      12862.5137056  13159.11968156 12829.45098706 13170.80203524\n",
    "#      12887.40786216 12865.17355183 12775.07284143 13219.37530298\n",
    "#      12629.2226568  13337.91849373 12739.52346283 13126.33611794\n",
    "#      12973.85461498 13142.88021733 12769.79374444 12892.50038965\n",
    "#      12756.68478372 13331.63329016 12815.43654144 13258.23586064\n",
    "#      12873.13341915 13089.28156523 12816.48535762 13246.32550649\n",
    "#      12904.9296742  12894.91374903 12792.88232696 13576.56582084\n",
    "#      12693.39705421 13276.45955528]\n",
    "#     47\n",
    "    \n",
    "\n",
    "    return int(i_star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1034a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def QBC(dim_h, dim_in, device, X_Ls, X_U, instance_i, epochs,load_model:str=None):\n",
    "    \n",
    "    # populate data matrix with dimensions: number of committee members, number of test patients * number of poses per patient, number of nodes, number of classes\n",
    "    num_cmt = len(X_Ls)\n",
    "    num_test_patient = len(X_U)\n",
    "    num_patient_poses = len(X_U[0])\n",
    "    probs_cmt = np.zeros((num_cmt, num_test_patient * num_patient_poses, X_U[0][0].num_nodes, 12))\n",
    "    # print('probs_cmt.shape',probs_cmt.shape)\n",
    "    \n",
    "    val_accs = np.zeros((num_cmt))\n",
    "    \n",
    "    start_time_2 = time.time()\n",
    "    \n",
    "    # For each committee, 1) train, 2) test, 3) use log softmax for every pose to quantify disagreement among committee\n",
    "    for i, x_l in enumerate(X_Ls):\n",
    "        print(f'\\n>>>>> Entering committe member {i+1}...')\n",
    "        \n",
    "        model_fname = f'/al_instance{instance_i}_cmt{i}'\n",
    "        model_path = model_dir+model_fname\n",
    "        \n",
    "        if load_model == None:\n",
    "            model = gf.GraphSAGE(dim_h, dim_in, 12)\n",
    "            model = DataParallel(model)\n",
    "            model.to(device)\n",
    "        \n",
    "        ################################\n",
    "        #split data in train and validation\n",
    "        #convert lists of lists\n",
    "        \n",
    "            print(len(x_l)) # 50 for 5 with 10 poses; 15 for 5 training patients with 3 poses each\n",
    "            print(type(x_l[0])) # graph torch_geometric Data\n",
    "        \n",
    "            val_train_split = len(x_l)//3\n",
    "            # print(val_train_split) # 16\n",
    "            val_lists = x_l[:val_train_split]\n",
    "            # print(len(val_lists)) # 16\n",
    "            # print(len(val_lists[0])) # 4\n",
    "            val_list = [patient_poses for patient_poses in val_lists]\n",
    "            # print(len(val_list)) # 16\n",
    "            # print(type(val_list[0])) # graph torch_geometric Data\n",
    "            train_lists = x_l[val_train_split:]\n",
    "            # print(len(train_lists)) # 34\n",
    "            # print(len(train_lists[0])) # 4\n",
    "            train_list = [patient_poses for patient_poses in train_lists]\n",
    "            # print(len(train_list)) # 34\n",
    "            # print(type(train_list[0])) # graph torch_geometric Data\n",
    "            test_list = [pose for patient_poses in X_U for pose in patient_poses]\n",
    "        \n",
    "        else:\n",
    "            train_list, val_list, test_list, model = load_old_save(model_path, load_model, num_poses)\n",
    "            model.to(device)\n",
    "            \n",
    "        # print('Constructing training data.')\n",
    "        # train_batch = Batch.from_data_list(train_list)\n",
    "        # train_data = DataLoader(train_batch, batch_size=1)\n",
    "        train_data = DataListLoader(train_list, batch_size=6, shuffle=True)\n",
    "\n",
    "        # print('Constructing validation data.')\n",
    "        # val_batch = Batch.from_data_list(val_list)\n",
    "        # val_data = DataLoader(val_batch, batch_size=1)\n",
    "        val_data = DataListLoader(val_list, batch_size=6, shuffle=True)\n",
    "\n",
    "        print('Training model...')\n",
    "        start_time = time.time()\n",
    "        # train_loss, train_acc, model = gf.train_model(model, device, train_data, val_data,epochs=epochs)\n",
    "        train_loss, train_acc, train_iou, train_dice, model = gf.train_model(model, device, train_data, val_data,epochs=epochs)\n",
    "    \n",
    "        # val_accs[i] = train_acc\n",
    "#         train_accs[instance_i] = train_acc\n",
    "        print(f'Total time taken for training: {time.time()-start_time:.4f}')\n",
    "\n",
    "        # print('Saving model...')\n",
    "\n",
    "        # gf.save_model(model, model_path)\n",
    "\n",
    "        # print('Constructing test data.')\n",
    "        # test_data = DataLoader(test_list)\n",
    "        test_data = DataListLoader(test_list, batch_size=6, shuffle=True)\n",
    "\n",
    "        # print('Running test graphs.')\n",
    "        # loss, acc, predict_probs = gf.test_model(model, device, test_data)\n",
    "        loss, acc, iou, dice, predict_probs = gf.test_model(model, device, test_data)\n",
    "#         print('Checking nans in predictions: where nans:', np.argwhere(np.isnan(predict_probs.cpu())))\n",
    "#         print('predict_probs.shape', predict_probs.shape) # [50, 6890, 12] for 5 patients and 10 poses\n",
    "#         print('predict_probs(0,:5,:)',predict_probs[0,:5,:]) # values are between 0 and 1\n",
    "        \n",
    "        probs_cmt[i,:,:,:] = predict_probs.cpu()\n",
    "\n",
    "        # print(f'Testing loss: {loss:.2f}')\n",
    "        # print(f'Testing accuracy: {acc:.2f}')\n",
    "        # print('-'*40)\n",
    "    \n",
    "    print(f'Total time for training and testing committee memmber {i+1}: {time.time()-start_time_2:.4f} ')\n",
    "    \n",
    "    # Consensus is the average across committee\n",
    "    # print('probs_cmt.shape = ', probs_cmt.shape) # (3, 50, 6890, 12) for 3 committe members\n",
    "    consensus = np.mean(probs_cmt, axis=0)\n",
    "    # print(f'percent zeros in consensus: {np.sum(consensus == 0) / consensus.size}')\n",
    "#     print('consensus.shape',consensus.shape) # [50, 6890, 12] for test case with 5 patients and 10 poses\n",
    "    \n",
    "    xstar_i = QuantifyLargestDisagreement(probs_cmt, consensus, num_patient_poses)\n",
    "    print(xstar_i)\n",
    "\n",
    "    return xstar_i, np.mean(val_accs), acc, consensus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2f0ba7",
   "metadata": {},
   "source": [
    "## Using MC dropout\n",
    "\n",
    "**Number of repeats** Number of repeats for test run is 2\n",
    "\n",
    "**Dropout probability** The probability of dropouts is 0.3\n",
    "\n",
    "**Point selection** Entropy is used to select the next instance  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e73995dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MC_dropout(dim_h, dim_in, device, X_L, X_U, num_poses, epochs,mc_prob,mc_repeats,load_model:str=None):\n",
    "    val_train_split = len(X_L)//3\n",
    "    val_lists = X_L[:val_train_split]\n",
    "    val_list = [pose for patient_poses in val_lists for pose in patient_poses]\n",
    "    train_lists = X_L[val_train_split:]\n",
    "    train_list = [pose for patient_poses in train_lists for pose in patient_poses]\n",
    "    test_list = [pose for patient_poses in X_U for pose in patient_poses]\n",
    "    \n",
    "    model = gf.GraphSAGE_mc(dim_h, dim_in, 12, mc_prob=mc_prob)\n",
    "    model = DataParallel(model)\n",
    "    model.to(device)\n",
    "\n",
    "    # print('Constructing training data.')\n",
    "    # train_batch = Batch.from_data_list(train_list)\n",
    "    # train_data = DataLoader(train_batch, batch_size=1)\n",
    "    train_data = DataListLoader(train_list, batch_size=6, shuffle=True)\n",
    "\n",
    "    # print('Constructing validation data.')\t\n",
    "    # val_batch = Batch.from_data_list(val_list)\n",
    "    # val_data = DataLoader(val_batch, batch_size=1)\n",
    "    val_data = DataListLoader(val_list, batch_size=6, shuffle=True)\n",
    "\n",
    "\n",
    "    print('Training model...')\n",
    "    start_time = time.time()\n",
    "    # _, _, model = gf.train_model(model, device, train_data, val_data, epochs=epochs)\n",
    "    train_loss, train_acc, train_iou, train_dice, model = gf.train_model(model, device, train_data, val_data,epochs=epochs)\n",
    "    # train_accs[instance_i] = train_acc\n",
    "    print(f'Total time taken for training: {time.time()-start_time:.4f} seconds')\n",
    "\n",
    "    # print('Constructing test data.')\n",
    "    # test_data = DataLoader(test_list)\n",
    "    test_data = DataListLoader(test_list, batch_size=6, shuffle=True)\n",
    "\n",
    "    #Repeat test runs\n",
    "    print('Running test graphs.')\n",
    "    # acc_list = []\n",
    "    predict_probs_list = []\n",
    "    start_time2 = time.time()\n",
    "    for j in range(mc_repeats):\n",
    "        # loss, acc, predict_probs = gf.test_model(model, device, test_data)\n",
    "        loss, acc, iou, dice, predict_probs = gf.test_model(model, device, test_data)\n",
    "        # acc_list.append(acc)\n",
    "        predict_probs_list.append(predict_probs.cpu())\n",
    "    \n",
    "    print(f'Total time taken for perform MC predictions: {time.time()-start_time2:.4f} seconds')\n",
    "\n",
    "    # print(f'Testing loss: {loss:.2f}')\n",
    "    # print(f'Testing accuracy: {acc:.2f}')\n",
    "    # print(f'Testing iou: {iou:.2f}')\n",
    "    # print(f'Testing dice: {dice:.2f}')\n",
    "    # print('-'*40)\n",
    "\n",
    "    #produces mean value array of (# nodes, # classes)\n",
    "    predict_probs_arr = np.mean(np.stack(predict_probs_list, axis=0), axis=0)\n",
    "    # print('predict_probs_arr.shape ', predict_probs_arr.shape)\n",
    "    \n",
    "    #get best patient\n",
    "    istar = Entropy(predict_probs_arr, num_poses)\n",
    "    # istar = Entropy(predict_probs_arr, 10)\n",
    "\n",
    "    return istar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95e74848",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Entropy(probs_a, num_poses):\n",
    "    '''\n",
    "    Calculate best instance to select using entropy. \n",
    "    Paramters:\n",
    "        probs_a: (u, c) numpy array\n",
    "            u is the number of unlabeled points and c is the number of classes\n",
    "    Returns:\n",
    "        best_patient: int\n",
    "            Index of best points in the unlabeled dataset\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # probs_a dimensions: number of unlabeled poses, number of nodes, number of classes\n",
    "    entropy = np.sum(probs_a*np.log(probs_a), axis=-1)\n",
    "    entropy = -np.sum(entropy, axis=-1)\n",
    "    best_choice = np.argmax(entropy)\n",
    "    best_patient = best_choice // num_poses\n",
    "#     best_patient = best_choice   \n",
    "    \n",
    "    return best_patient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ae3c05",
   "metadata": {},
   "source": [
    "# Load data\n",
    "\n",
    "Load MPI Faust meshes stored as .json dictionaries representing torch_geometric Data objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "47e96479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3381\n",
      "[10, 10, 10, 10, 10, 10, 10, 10, 10, 10]\n"
     ]
    }
   ],
   "source": [
    "num_poses = 10\n",
    "\n",
    "gt_path = \"segmentations.npz\"\n",
    "data = np.load(gt_path)\n",
    "node_labels = data['segmentation_labels'].reshape(-1,)\n",
    "\n",
    "save_dir = \"/Data\"\n",
    "data_path = \"./data_dicts\"\n",
    "data_light_path = \"./data_dicts_light\"\n",
    "\n",
    "# graph_list = []\n",
    "graph_files = os.listdir(data_path)\n",
    "# print(len(graph_files))\n",
    "\n",
    "\n",
    "# graph = gf.construct_graph(data_path, 0, node_labels)\n",
    "# pruned_edge_index = ep.graph_pruning(graph)\n",
    "# for i, g in enumerate(graph_files):\n",
    "#     graph = gf.construct_graph(data_path, i, node_labels, edges=pruned_edge_index)\n",
    "#     graph_list.append(graph)\n",
    "\n",
    "\n",
    "nodes = np.random.choice(range(6890),size=int(6890*0.5), replace=False)\n",
    "set_nodes = set(nodes)\n",
    "\n",
    "with open(os.path.join(data_path, f'data_0.json'), 'r') as f:\n",
    "    graph_dict = json.load(f)\n",
    "edges = graph_dict['edge_index'] # list of 2 lists\n",
    "\n",
    "light_edges = [[],[]]\n",
    "for u,v in zip(edges[0], edges[1]):\n",
    "    if (u in set_nodes) and (v in set_nodes):\n",
    "#         print((u in set_nodes))\n",
    "#         print('here')\n",
    "        light_edges[0].append(u)\n",
    "        light_edges[1].append(v)\n",
    "        \n",
    "temp_list = light_edges[0] + light_edges[1]\n",
    "non_lone_nodes = list(np.unique(temp_list))\n",
    "temp_list_mapped = [non_lone_nodes.index(n) for n in temp_list]\n",
    "light_edges[0] = temp_list_mapped[:len(temp_list)//2]\n",
    "light_edges[1] = temp_list_mapped[len(temp_list)//2:]\n",
    "\n",
    "new_node_labels = node_labels[non_lone_nodes]\n",
    "print(len(new_node_labels))\n",
    "\n",
    "for i, graph in enumerate(graph_files):\n",
    "    with open(os.path.join(data_path, f'data_{i}.json'), 'r') as f:\n",
    "        graph_dict = json.load(f)\n",
    "    graph_l = {}\n",
    "    graph_l['x'] = [graph_dict['x'][x] for x in non_lone_nodes]\n",
    "    graph_l['face'] = [] \n",
    "    graph_l['edge_index'] = light_edges\n",
    "#     print(len(graph_dict['x']))\n",
    "    # save json\n",
    "    with open(os.path.join(data_light_path, f'data_{i}.json'), 'w') as f:\n",
    "        json.dump(graph_l, f)\n",
    "\n",
    "graph_list = []       \n",
    "graph_files = os.listdir(data_light_path)\n",
    "for i, graph in enumerate(graph_files):\n",
    "    \n",
    "    light_g = gf.construct_graph(data_light_path, i, new_node_labels)\n",
    "    \n",
    "    graph_list.append(light_g)\n",
    "\n",
    "\n",
    "\n",
    "########## Patients ###################\n",
    "# X = [graph_list[i*10:(i*10)+num_poses] for i in range(10)]\n",
    "\n",
    "########## Poses ###################\n",
    "X = []\n",
    "for i in range(10):\n",
    "    pose_list = []\n",
    "    for j in range(10):\n",
    "        pose_list.append(graph_list[i+(num_poses*j)])\n",
    "    X.append(pose_list)\n",
    "\n",
    "\n",
    "lens = [len(X[get_i]) for get_i in range(len(X))]\n",
    "print(lens)\n",
    "# X = [poses for poses in X[:8]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf910ba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 10 20 30 40 50 60 70 80 90 \n",
      "1 11 21 31 41 51 61 71 81 91 \n",
      "2 12 22 32 42 52 62 72 82 92 \n",
      "3 13 23 33 43 53 63 73 83 93 \n",
      "4 14 24 34 44 54 64 74 84 94 \n",
      "5 15 25 35 45 55 65 75 85 95 \n",
      "6 16 26 36 46 56 66 76 86 96 \n",
      "7 17 27 37 47 57 67 77 87 97 \n",
      "8 18 28 38 48 58 68 78 88 98 \n",
      "9 19 29 39 49 59 69 79 89 99 \n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    x = ''\n",
    "    for j in range(10):\n",
    "        x += f'{i+(10*j)} '\n",
    "        # print(i+(10*j))\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c5a9bc",
   "metadata": {},
   "source": [
    "# Main\n",
    "\n",
    "Main simulation has 5 rounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "28115f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "num_runs = 5\n",
    "# n = X.shape[0]\n",
    "start_n = 3\n",
    "end_n = 9\n",
    "epochs = 25\n",
    "dropout = 0.3\n",
    "mc_repeats = 10\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "num_instances = end_n - start_n\n",
    "\n",
    "train_stats = np.zeros((num_runs, 4, num_instances + 1))\n",
    "unobs_stats = np.zeros((num_runs, 4, num_instances + 1))\n",
    "\n",
    "train_stats_qbc = np.zeros((num_runs, 4, num_instances + 1))\n",
    "unobs_stats_qbc = np.zeros((num_runs, 4, num_instances + 1))\n",
    "\n",
    "train_stats_mc = np.zeros((num_runs, 4, num_instances + 1))\n",
    "unobs_stats_mc = np.zeros((num_runs, 4, num_instances + 1))\n",
    "\n",
    "# train_accs = np.zeros((num_runs, num_instances + 1))\n",
    "# unobs_accs = np.zeros((num_runs, num_instances + 1))\n",
    "\n",
    "# train_accs_qbc = np.zeros((num_runs, num_instances + 1))\n",
    "# unobs_accs_qbc = np.zeros((num_runs, num_instances + 1))\n",
    "\n",
    "# train_accs_mc = np.zeros((num_runs, num_instances + 1))\n",
    "# unobs_accs_mc = np.zeros((num_runs, num_instances + 1))\n",
    "\n",
    "old_stdout = sys.stdout\n",
    "\n",
    "sys.stdout = open(f\"./logs/passive/passive_log_{start_n}_{end_n}.log\", 'w')\n",
    "# sys.stdout = open(f\"./logs/qbc/qbc_log_{start_n}_{end_n}.log\", 'w')\n",
    "# sys.stdout = open(f\"./logs/mc/mc_log_{start_n}_{end_n}.log\", 'w')\n",
    "\n",
    "begin_time = time.time()\n",
    "\n",
    "for run_i in range(num_runs):\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    np.random.seed(my_seed + run_i)\n",
    "    torch.manual_seed(my_seed + run_i)\n",
    "    torch.cuda.manual_seed(my_seed + run_i)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    print()\n",
    "    print('^'*40)\n",
    "    print(f'Entering active learning Run {run_i+1}..........')\n",
    "\n",
    "    rand_i = np.arange(len(X))\n",
    "    np.random.shuffle(rand_i)\n",
    "    X = [X[get_i] for get_i in rand_i]\n",
    "\n",
    "    # dim_in = graph.num_features\t\t\n",
    "    # dim_h = graph.num_nodes\n",
    "    dim_in = light_g.num_features\t\t\n",
    "    dim_h = light_g.num_nodes\n",
    "\n",
    "    gif_dir = './save_gif2'\n",
    "    if not os.path.exists(gif_dir):\n",
    "        os.makedirs(gif_dir)\n",
    "\n",
    "    # Passive Learning\n",
    "    \n",
    "    model_dir = f'./models/models_al_passive_debug2/al_run{run_i+1}'\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "    pred_dir = f'./preds/preds_al_passive_debug3/al_run{run_i+1}'\n",
    "    if not os.path.exists(pred_dir):\n",
    "        os.makedirs(pred_dir)\n",
    "    train_pas, unobs_pas = NewOnlineLearning(dim_h, dim_in, model_dir, pred_dir, gif_dir, start_n, end_n, num_instances, num_poses, None, X,epochs,rand_i)\n",
    "    # print(train_accs_j.shape)\n",
    "    train_stats[run_i] = train_pas\n",
    "    unobs_stats[run_i] = unobs_pas\n",
    "    print(\"\\n$$$$\")\n",
    "    print(f\"Passive Run {run_i+1} train, unobs stats\")\n",
    "    print(train_pas)\n",
    "    print(unobs_pas)\n",
    "    print(\"$$$$\\n\")\n",
    "\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    np.save(f'stats/passive_{start_n}_{end_n}_train_stats.npy',train_stats)\n",
    "    np.save(f'stats/passive_{start_n}_{end_n}_unobs_stats.npy',unobs_stats)\n",
    "    \n",
    "    # Active Learning with query by committee\n",
    "    \n",
    "    model_dir = f'./models/models_al_qbc_debug3/al_run{run_i+1}'\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "    pred_dir = f'./preds/preds_al_qbc_debug3/al_run{run_i+1}'\n",
    "    if not os.path.exists(pred_dir):\n",
    "        os.makedirs(pred_dir)\n",
    "    qbc_al_train, qbc_al_unobs = NewOnlineLearning(dim_h, dim_in, model_dir, pred_dir, gif_dir, start_n, end_n, num_instances, num_poses, \"qbc\", X, epochs, rand_i)\n",
    "    train_stats_qbc[run_i] = qbc_al_train\n",
    "    unobs_stats_qbc[run_i] = qbc_al_unobs\n",
    "\n",
    "    print(\"\\n$$$$\")\n",
    "    print(f\"QBC Run {run_i+1} train, unobs stats\")\n",
    "    print(qbc_al_train)\n",
    "    print(qbc_al_unobs)\n",
    "    print(\"$$$$\\n\")\n",
    "\n",
    "    np.save(f'stats/qbc_{start_n}_{end_n}_train_stats.npy',train_stats_qbc)\n",
    "    np.save(f'stats/qbc_{start_n}_{end_n}_unobs_stats.npy',unobs_stats_qbc)\n",
    "    \n",
    "    sys.stdout.flush()\n",
    "\n",
    "    \n",
    "    # Active Learning with mc_dropout\n",
    "    model_dir = f'./models/models_al_mc_debug3/al_run{run_i+1}'\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "    pred_dir = f'./preds/preds_al_mc_debug3/al_run{run_i+1}'\n",
    "    if not os.path.exists(pred_dir):\n",
    "        os.makedirs(pred_dir)\n",
    "    # train_accs, train_dices, train_losses, train_ious, unobs_accs, unobs_dices, unobs_losses, unobs_ious\n",
    "    mc_al_train, mc_al_unobs = NewOnlineLearning(dim_h, dim_in, model_dir, pred_dir, gif_dir, start_n, end_n, \n",
    "                                                           num_instances, num_poses, \"mc\", X, epochs,rand_i,\n",
    "                                                           mc_prob=dropout,mc_repeats=mc_repeats)\n",
    "    # mc_al_train_accs, mc_al_unobs_accs, probs = OnlineLearning_mc(dim_h, dim_in, model_dir, pred_dir, gif_dir, start_n, end_n, num_instances, \"mc\", X, num_poses,\n",
    "    #                                                       mc_prob=0.3,\n",
    "    #                                                        mc_repeats=mc_repeats)\n",
    "    train_stats_mc[run_i] = mc_al_train\n",
    "    unobs_stats_mc[run_i] = mc_al_unobs\n",
    "\n",
    "    print(\"\\n$$$$\")\n",
    "    print(f\"MC Run {run_i+1} train, unobs stats\")\n",
    "    print(mc_al_train)\n",
    "    print(mc_al_unobs)\n",
    "    print(\"$$$$\\n\")\n",
    "\n",
    "    np.save(f'stats/mc_{start_n}_{end_n}_train_stats.npy',train_stats_mc)\n",
    "    np.save(f'stats/mc_{start_n}_{end_n}_unobs_stats.npy',unobs_stats_mc)\n",
    "    \n",
    "    # sys.stdout.flush()\n",
    "#     OnlineLearning_fake(dim_h, dim_in, model_dir, pred_dir, start_n, end_n, num_instances, None, X, run_i)\n",
    "    \n",
    "print(f'Total time: {(time.time()-begin_time)/60:.4f} mins')\n",
    "\n",
    "sys.stdout.close()\n",
    "sys.stdout = old_stdout\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3a3c5603",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'graphClasses' from '/home/ec2-user/finalproj/02750FinalProject/graphClasses.py'>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(vf)\n",
    "importlib.reload(gf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "c181678a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gif_dir = 'save_gif'\n",
    "imgobj = vf.ImageObject(gif_dir,[])\n",
    "imgobj.create_GIF(4, 7,'qbc', movie_name='movie3_qbc.gif')\n",
    "imgobj.create_GIF(4, 7,'None', movie_name='movie3_passive.gif')\n",
    "imgobj.create_GIF(4, 7,'mc', movie_name='movie3_mc.gif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd700c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0ecd42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "232045b7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'num_instances' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m fig \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39mfigure(figsize \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n\u001b[1;32m      9\u001b[0m x_labels\u001b[38;5;241m=\u001b[39m[]\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m,\u001b[43mnum_instances\u001b[49m):\n\u001b[1;32m     11\u001b[0m     x_labels\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mstr\u001b[39m(i\u001b[38;5;241m+\u001b[39mstart_n))\n\u001b[1;32m     13\u001b[0m datas \u001b[38;5;241m=\u001b[39m [train_accs, unobs_accs, train_accs_qbc, unobs_accs_qbc, train_accs_mc, unobs_accs_mc]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'num_instances' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "SAVE_DIR = 'outputs'\n",
    "fname = 'al_debug_mc8.png'\n",
    "\n",
    "if not os.path.isdir(SAVE_DIR):\n",
    "    os.makedirs(SAVE_DIR)\n",
    "\n",
    "fig = plt.figure(figsize = (10, 5))\n",
    "\n",
    "x_labels=[]\n",
    "for i in range(0,num_instances):\n",
    "    x_labels.append(str(i+start_n))\n",
    "\n",
    "datas = [train_accs, unobs_accs, train_accs_qbc, unobs_accs_qbc, train_accs_mc, unobs_accs_mc]\n",
    "labels = ['Passive training', 'Passive unobserved', 'Active:QBC training', 'Active:QBC unobserved', 'Active:MC training', 'Active:MC unobserved']\n",
    "colors = ['blue', 'xkcd:light blue', 'xkcd:green', 'xkcd:light green', 'xkcd:pink', 'xkcd:light pink']\n",
    "\n",
    "# datas = [train_accs, unobs_accs, train_accs_mc, unobs_accs_mc]\n",
    "# labels = ['Passive training', 'Passive unobserved', 'Active:MC training', 'Active:MC unobserved']\n",
    "# colors = ['blue', 'xkcd:light blue', 'xkcd:pink', 'xkcd:light pink']\n",
    "\n",
    "# # datas = [train_accs_qbc, unobs_accs_qbc]\n",
    "# # labels = ['Active:MC training', 'Active:MC unobserved']\n",
    "# # colors = ['xkcd:green', 'xkcd:light green']\n",
    "\n",
    "for d, l, c in zip(datas, labels, colors):\n",
    "    if len(d.shape) > 1:\n",
    "        y_plot = np.mean(d, axis=0)\n",
    "        sim_std = np.std(d, axis=0)\n",
    "    else:\n",
    "        y_plot = [np.mean(d)]*len(x_labels)\n",
    "        sim_std = [np.std(d)]*len(x_labels)\n",
    "    plt.plot(x_labels, y_plot, color =c, label=l)\n",
    "    plt.errorbar(x_labels, y_plot, yerr=sim_std, color=c, ecolor=c, alpha=0.5, capsize=3)\n",
    "\n",
    "\n",
    "plt.xlabel(\"Rounds\")\n",
    "# plt.xticks(fontsize=7,rotation=90)\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.title(\"Learning curves\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# plt.savefig(os.path.join(SAVE_DIR, fname))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "db12f7c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'visFunctions' from '/home/sofialima/02750FinalProject/visFunctions.py'>"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(vf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b4c4cdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def OnlineLearning_fake(dim_h, dim_in, model_dir, pred_dir, start_n, end_n, num_instances, querySelection, X, run_i):\n",
    "    train_accs = np.zeros((num_instances))\n",
    "    unobs_accs = np.zeros((num_instances))\n",
    "    \n",
    "    X_L, X_U = X[:start_n], X[start_n:]\n",
    "    \n",
    "    if run_i == 0:\n",
    "        save_path = r'./save_gif'\n",
    "        idx_list = list(range(start_n, end_n))\n",
    "        print(idx_list)\n",
    "        imgobj = ImageObject(save_path,idx_list)\n",
    "\n",
    "    for instance_i, split_i in enumerate(range(start_n,end_n)):\n",
    "        print(f'Entering instance {instance_i+1}..........')\n",
    "        \n",
    "        if load_model == False:\n",
    "            model = gf.GraphSAGE(dim_h, dim_in, 12)\n",
    "            model.to(device)\n",
    "            \n",
    "        else:\n",
    "            train_list, val_list, test_list, model = load_old_save(save_path, latest_model, num_poses)\n",
    "            model.to(device)\n",
    "            \n",
    "        if querySelection == None:\n",
    "            \n",
    "            if load_model == False:\n",
    "\n",
    "                # Passive learning\n",
    "                X_i = X[:split_i]\n",
    "            \n",
    "                #split data in test, train and validation\n",
    "                val_train_split = len(X_i)//3\n",
    "                val_lists = X_i[:val_train_split]\n",
    "                val_list = [pose for patient_poses in val_lists for pose in patient_poses]\n",
    "                train_lists = X_i[val_train_split:]\n",
    "                train_list = [pose for patient_poses in train_lists for pose in patient_poses]\n",
    "\n",
    "                test_lists = X[split_i:]\n",
    "                test_list = [pose for patient_poses in test_lists for pose in patient_poses]\n",
    "\n",
    "            \n",
    "            \n",
    "            print('Constructing training data.')\n",
    "            train_batch = Batch.from_data_list(train_list)\n",
    "            train_data = DataLoader(train_batch, batch_size=1)\n",
    "\n",
    "            print('Constructing validation data.')\t\n",
    "            val_batch = Batch.from_data_list(val_list)\n",
    "            val_data = DataLoader(val_batch, batch_size=1)\n",
    "            \n",
    "            \n",
    "            print('Constructing test data.')\n",
    "            test_data = DataLoader(test_list)\n",
    "            \n",
    "            print('Running test graphs.')\n",
    "            loss, acc, predict_probs = gf.test_model(model, device, test_data)\n",
    "            print(predict_probs.shape)\n",
    "            \n",
    "            if run_i == 0:\n",
    "                imgobj.create_images(test_list, predict_probs, instance_i, querySelection)\n",
    "                print('split_i ',split_i)\n",
    "                imgobj.track_index(split_i)\n",
    "\n",
    "#                 imgobj.track_index(instance_i)\n",
    "            \n",
    "\n",
    "            print(f'Testing loss: {loss:.2f}')\n",
    "            print(f'Testing accuracy: {acc:.2f}')\n",
    "            print('-'*40)\n",
    "            \n",
    "            unobs_accs[instance_i] = acc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

